---
title: "p8106 hw3"
author: "Nathalie Fadel"
date: "4/8/2019"
output: html_document
---

##Part A

###Import & view data
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
library(ISLR)
library(AppliedPredictiveModeling)
library(glmnet)
library(e1071)
library(pROC)
library(MASS)
library(mlbench)
library(class)
```

```{r}
data("Weekly")

summary(Weekly)
```

###Plots
```{r}
pairs(Weekly) 

transparentTheme(trans = .4)
featurePlot(x = Weekly[, 1:8], 
            y = Weekly$Direction,
            scales = list(x=list(relation="free"), 
                        y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

```

##Part B

###Logistic Regression
```{r}

glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
               data = Weekly, 
               family = binomial)

summary(glm.fit)
```
From the glm, we can see that the only significant predictor other than the intercept term is Lag 2.  

##Part C  

###Confusion matrix  
```{r}
probs = predict(glm.fit, type = "response")
preds = rep("Down", 1089)
preds[probs > 0.5] = "Up"
table(preds, Weekly$Direction)
confusionMatrix(data = as.factor(preds), reference = Weekly$Direction, positive = "Down")
```
Based on the confusion matrix, we can see that most of the cases go up (987/1089 cases) whereas in reality there are only 605/1089 that go up. This indicates that our prediction model does not predict direction well. However, we have a large proportion of true positives (557/605 = 0.921) but this comes at a cost of finding many false positives (430/987 = 0.436).   

##Part D

###ROC curves
```{r}
test.pred.prob  <- predict(glm.fit, newdata = Weekly, type = "response")
roc.glm <- roc(Weekly$Direction, test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```  
The AUC is 0.554. 

##Part E

```{r}
training.data = Weekly[Weekly$Year < 2009,]
test.data = Weekly[Weekly$Year > 2008,]
glm.fit2 = glm(Direction ~ Lag1 + Lag2, data = training.data, family = "binomial")
summary(glm.fit2)

test.pred.prob2 <- predict(glm.fit2, newdata = test.data, type = "response")
roc.glm2 <- roc(test.data$Direction, test.pred.prob2)
plot(roc.glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm2), col = 4, add = TRUE)
```
The AUC is 0.556  

##Part F

###LDA 
```{r}
lda.fit = lda(Direction ~ Lag1 + Lag2, data = training.data)
lda.fit
plot(lda.fit)

lda.pred <- predict(lda.fit, newdata = test.data)
head(lda.pred$posterior)

roc.lda <- roc(test.data$Direction, lda.pred$posterior[,2])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.lda), col = 4, add = TRUE)
```
The AUC is 0.557.  

###QDA
```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = training.data)
qda.fit

qda.pred <- predict(qda.fit, newdata = test.data)
head(qda.pred$posterior)

roc.qda <- roc(test.data$Direction, qda.pred$posterior[,2])
plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.qda), col = 4, add = TRUE)
```
The AUC is 0.529.  

##Part G  

###KNN
```{r}
set.seed(1)
train.X = cbind(training.data$Lag2)
test.X = cbind(test.data$Lag2)
train.Y = cbind(training.data$Direction)
knn.pred = knn(train.X, test.X, train.Y, k = 1)
table(knn.pred, test.data$Direction)

knn5.pred = knn(train.X, test.X, train.Y, k = 5)
table(knn5.pred, test.data$Direction)

set.seed(1)
trctrl <- trainControl(method = "repeatedcv", summaryFunction = twoClassSummary,
                     classProbs = TRUE, number = 10, repeats = 5)
knn.fit <- train(Direction ~ Lag1 + Lag2, data = training.data,
                   method = "knn",
                   preProcess = c("center","scale"),
                   trControl = trctrl,
                   metric = "ROC",
                    tuneLength = 10)
knn.fit
ggplot(knn.fit)
knn.fit$bestTune

knn.predict <- predict(knn.fit, newdata = test.data)
plot(knn.fit, print.thres = 0.5, type = "S")
```
The KNN model appears to be better at detecting true positives but worse at detecting true negatives. But the amount of true results overall increases as k gets larger. 
