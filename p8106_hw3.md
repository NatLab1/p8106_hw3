p8106 hw3
================
Nathalie Fadel
4/8/2019

Part A
------

### Import & view data

``` r
data("Weekly")

summary(Weekly)
```

    ##       Year           Lag1               Lag2               Lag3         
    ##  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  
    ##  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  
    ##  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  
    ##  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  
    ##  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  
    ##  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  
    ##       Lag4               Lag5              Volume       
    ##  Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747  
    ##  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202  
    ##  Median :  0.2380   Median :  0.2340   Median :1.00268  
    ##  Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462  
    ##  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373  
    ##  Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821  
    ##      Today          Direction 
    ##  Min.   :-18.1950   Down:484  
    ##  1st Qu.: -1.1540   Up  :605  
    ##  Median :  0.2410             
    ##  Mean   :  0.1499             
    ##  3rd Qu.:  1.4050             
    ##  Max.   : 12.0260

### Plots

``` r
pairs(Weekly) 
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-2-1.png)

``` r
transparentTheme(trans = .4)
featurePlot(x = Weekly[, 1:8], 
            y = Weekly$Direction,
            scales = list(x=list(relation="free"), 
                        y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-2-2.png)

Part B
------

### Logistic Regression

``` r
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
               data = Weekly, 
               family = binomial)

summary(glm.fit)
```

    ## 
    ## Call:
    ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
    ##     Volume, family = binomial, data = Weekly)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -1.6949  -1.2565   0.9913   1.0849   1.4579  
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(>|z|)   
    ## (Intercept)  0.26686    0.08593   3.106   0.0019 **
    ## Lag1        -0.04127    0.02641  -1.563   0.1181   
    ## Lag2         0.05844    0.02686   2.175   0.0296 * 
    ## Lag3        -0.01606    0.02666  -0.602   0.5469   
    ## Lag4        -0.02779    0.02646  -1.050   0.2937   
    ## Lag5        -0.01447    0.02638  -0.549   0.5833   
    ## Volume      -0.02274    0.03690  -0.616   0.5377   
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 1496.2  on 1088  degrees of freedom
    ## Residual deviance: 1486.4  on 1082  degrees of freedom
    ## AIC: 1500.4
    ## 
    ## Number of Fisher Scoring iterations: 4

From the glm, we can see that the only significant predictor other than the intercept term is Lag 2.

Part C
------

### Confusion matrix

``` r
probs = predict(glm.fit, type = "response")
preds = rep("Down", 1089)
preds[probs > 0.5] = "Up"
table(preds, Weekly$Direction)
```

    ##       
    ## preds  Down  Up
    ##   Down   54  48
    ##   Up    430 557

``` r
confusionMatrix(data = as.factor(preds), reference = Weekly$Direction, positive = "Down")
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction Down  Up
    ##       Down   54  48
    ##       Up    430 557
    ##                                          
    ##                Accuracy : 0.5611         
    ##                  95% CI : (0.531, 0.5908)
    ##     No Information Rate : 0.5556         
    ##     P-Value [Acc > NIR] : 0.369          
    ##                                          
    ##                   Kappa : 0.035          
    ##  Mcnemar's Test P-Value : <2e-16         
    ##                                          
    ##             Sensitivity : 0.11157        
    ##             Specificity : 0.92066        
    ##          Pos Pred Value : 0.52941        
    ##          Neg Pred Value : 0.56434        
    ##              Prevalence : 0.44444        
    ##          Detection Rate : 0.04959        
    ##    Detection Prevalence : 0.09366        
    ##       Balanced Accuracy : 0.51612        
    ##                                          
    ##        'Positive' Class : Down           
    ## 

Based on the confusion matrix, we can see that most of the cases go up (987/1089 cases) whereas in reality there are only 605/1089 that go up. This indicates that our prediction model does not predict direction well. However, we have a large proportion of true positives (557/605 = 0.921) but this comes at a cost of finding many false positives (430/987 = 0.436).

Part D
------

### ROC curves

``` r
test.pred.prob  <- predict(glm.fit, newdata = Weekly, type = "response")
roc.glm <- roc(Weekly$Direction, test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-5-1.png) The AUC is 0.554.

Part E
------

``` r
training.data = Weekly[Weekly$Year < 2009,]
test.data = Weekly[Weekly$Year > 2008,]
glm.fit2 = glm(Direction ~ Lag1 + Lag2, data = training.data, family = "binomial")
summary(glm.fit2)
```

    ## 
    ## Call:
    ## glm(formula = Direction ~ Lag1 + Lag2, family = "binomial", data = training.data)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -1.6149  -1.2565   0.9989   1.0875   1.5330  
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(>|z|)   
    ## (Intercept)  0.21109    0.06456   3.269  0.00108 **
    ## Lag1        -0.05421    0.02886  -1.878  0.06034 . 
    ## Lag2         0.05384    0.02905   1.854  0.06379 . 
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 1354.7  on 984  degrees of freedom
    ## Residual deviance: 1347.0  on 982  degrees of freedom
    ## AIC: 1353
    ## 
    ## Number of Fisher Scoring iterations: 4

``` r
test.pred.prob2 <- predict(glm.fit2, newdata = test.data, type = "response")
roc.glm2 <- roc(test.data$Direction, test.pred.prob2)
plot(roc.glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm2), col = 4, add = TRUE)
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-6-1.png) The AUC is 0.556

Part F
------

### LDA

``` r
lda.fit = lda(Direction ~ Lag1 + Lag2, data = training.data)
lda.fit
```

    ## Call:
    ## lda(Direction ~ Lag1 + Lag2, data = training.data)
    ## 
    ## Prior probabilities of groups:
    ##      Down        Up 
    ## 0.4477157 0.5522843 
    ## 
    ## Group means:
    ##              Lag1        Lag2
    ## Down  0.289444444 -0.03568254
    ## Up   -0.009213235  0.26036581
    ## 
    ## Coefficients of linear discriminants:
    ##             LD1
    ## Lag1 -0.3013148
    ## Lag2  0.2982579

``` r
plot(lda.fit)
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-7-1.png)

``` r
lda.pred <- predict(lda.fit, newdata = test.data)
head(lda.pred$posterior)
```

    ##          Down        Up
    ## 986 0.5602039 0.4397961
    ## 987 0.3079163 0.6920837
    ## 988 0.4458032 0.5541968
    ## 989 0.4785107 0.5214893
    ## 990 0.4657943 0.5342057
    ## 991 0.5262907 0.4737093

``` r
roc.lda <- roc(test.data$Direction, lda.pred$posterior[,2])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.lda), col = 4, add = TRUE)
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-7-2.png) The AUC is 0.557.

### QDA

``` r
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = training.data)
qda.fit
```

    ## Call:
    ## qda(Direction ~ Lag1 + Lag2, data = training.data)
    ## 
    ## Prior probabilities of groups:
    ##      Down        Up 
    ## 0.4477157 0.5522843 
    ## 
    ## Group means:
    ##              Lag1        Lag2
    ## Down  0.289444444 -0.03568254
    ## Up   -0.009213235  0.26036581

``` r
qda.pred <- predict(qda.fit, newdata = test.data)
head(qda.pred$posterior)
```

    ##          Down        Up
    ## 986 0.5436205 0.4563795
    ## 987 0.3528814 0.6471186
    ## 988 0.2227273 0.7772727
    ## 989 0.3483016 0.6516984
    ## 990 0.4598550 0.5401450
    ## 991 0.5119613 0.4880387

``` r
roc.qda <- roc(test.data$Direction, qda.pred$posterior[,2])
plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.qda), col = 4, add = TRUE)
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-8-1.png) The AUC is 0.529.

Part G
------

### KNN

``` r
set.seed(1)
train.X = cbind(training.data$Lag2)
test.X = cbind(test.data$Lag2)
train.Y = cbind(training.data$Direction)
knn.pred = knn(train.X, test.X, train.Y, k = 1)
table(knn.pred, test.data$Direction)
```

    ##         
    ## knn.pred Down Up
    ##        1   21 30
    ##        2   22 31

``` r
knn5.pred = knn(train.X, test.X, train.Y, k = 5)
table(knn5.pred, test.data$Direction)
```

    ##          
    ## knn5.pred Down Up
    ##         1   15 20
    ##         2   28 41

``` r
set.seed(1)
trctrl <- trainControl(method = "repeatedcv", summaryFunction = twoClassSummary,
                     classProbs = TRUE, number = 10, repeats = 5)
knn.fit <- train(Direction ~ Lag1 + Lag2, data = training.data,
                   method = "knn",
                   preProcess = c("center","scale"),
                   trControl = trctrl,
                   metric = "ROC",
                    tuneLength = 10)
knn.fit
```

    ## k-Nearest Neighbors 
    ## 
    ## 985 samples
    ##   2 predictor
    ##   2 classes: 'Down', 'Up' 
    ## 
    ## Pre-processing: centered (2), scaled (2) 
    ## Resampling: Cross-Validated (10 fold, repeated 5 times) 
    ## Summary of sample sizes: 887, 887, 886, 886, 886, 887, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   k   ROC        Sens       Spec     
    ##    5  0.5444665  0.4471919  0.6231044
    ##    7  0.5475372  0.4281515  0.6235017
    ##    9  0.5440455  0.4027980  0.6581347
    ##   11  0.5358450  0.3873232  0.6548148
    ##   13  0.5241345  0.3727172  0.6624983
    ##   15  0.5212281  0.3690606  0.6647003
    ##   17  0.5102385  0.3559192  0.6540067
    ##   19  0.5061427  0.3482424  0.6550640
    ##   21  0.5016306  0.3527374  0.6440337
    ##   23  0.5012484  0.3527475  0.6421684
    ## 
    ## ROC was used to select the optimal model using the largest value.
    ## The final value used for the model was k = 7.

``` r
ggplot(knn.fit)
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-9-1.png)

``` r
knn.fit$bestTune
```

    ##   k
    ## 2 7

``` r
knn.predict <- predict(knn.fit, newdata = test.data)
plot(knn.fit, print.thres = 0.5, type = "S")
```

![](p8106_hw3_files/figure-markdown_github/unnamed-chunk-9-2.png) The KNN model appears to be better at detecting true positives but worse at detecting true negatives. But the amount of true results overall increases as k gets larger.
